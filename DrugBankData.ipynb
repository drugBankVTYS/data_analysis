{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dab9adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\makine_ogrenimi_ve_uygulamalari\\let1\\env\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\makine_ogrenimi_ve_uygulamalari\\let1\\env\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7016d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e246c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\makine_ogrenimi_ve_uygulamalari\\let1\\env\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\makine_ogrenimi_ve_uygulamalari\\let1\\env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94dfc93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e27a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tqd (from versions: none)\n",
      "ERROR: No matching distribution found for tqd\n"
     ]
    }
   ],
   "source": [
    "!pip install tqd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66b8071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining tqdm from git+https://github.com/tqdm/tqdm.git@master#egg=tqdm\n",
      "  Updating c:\\makine_ogrenimi_ve_uygulamalari\\let1\\src\\tqdm clone (to revision master)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\makine_ogrenimi_ve_uygulamalari\\let1\\env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Building wheels for collected packages: tqdm\n",
      "  Building editable for tqdm (pyproject.toml): started\n",
      "  Building editable for tqdm (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for tqdm: filename=tqdm-4.66.1-0.editable-py3-none-any.whl size=22420 sha256=19e88399ff07207a871fb4809b026f4ecb99f8812a3fa40b81215e0926b982dd\n",
      "  Stored in directory: C:\\Users\\ozkan\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-kl1n8uzt\\wheels\\8c\\27\\a4\\d688c0e45f7f8f21e9ce997f4e8a3d9e29b53e568f3a64de2b\n",
      "Successfully built tqdm\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "Successfully installed tqdm-4.66.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git fetch -q --tags\n",
      "  Running command git reset --hard -q 4c956c20b83be4312460fc0c4812eeb3fef5e7df\n"
     ]
    }
   ],
   "source": [
    "!pip install -e git+https://github.com/tqdm/tqdm.git@master#egg=tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "819173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87f4593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drug:\n",
    "    \"\"\"\n",
    "    docstring for Drug.\n",
    "    btk akedemi algortima analizi \n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "\n",
    "        self.id = features['id']\n",
    "        self.name = features['name']\n",
    "        self.state = features['state']\n",
    "        self.kingdom = features['kgd']\n",
    "        self.superclass = features['sclass']\n",
    "        self.interaction = features['itrc']\n",
    "        self.pathways = features['pathways']\n",
    "        self.toxicity = features['toxicity']\n",
    "        self.food_interactions = features['food_interactions']\n",
    "        self.target = []\n",
    "\n",
    "    def getDrugfeatures(self):\n",
    "        drug_dict = {\"drug_id\":self.id,\n",
    "                    \"drug_name\":self.name,\n",
    "                    \"drug_state\":self.state,\n",
    "                    \"drug_kingdom\":self.kingdom,\n",
    "                    \"drug_superclass\":self.superclass,\n",
    "                    \"drug_interactions\":self.interaction,\n",
    "                    \"drug_pathways\":self.pathways,\n",
    "                    \"drug_toxicity\":self.toxicity,\n",
    "                    \"drug_food_interactions\":self.food_interactions}\n",
    "        return drug_dict\n",
    "\n",
    "    def addTarget(self, feature_target):\n",
    "        self.target.append(feature_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2efec658",
   "metadata": {},
   "outputs": [],
   "source": [
    "dB_file = 'full_database.xml'\n",
    "organism = 'Humans'\n",
    "saveFile = 'drugBank_v515.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a273dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_sentences(sentencesV):\n",
    "    \n",
    "    doc = nlp(sentencesV)\n",
    "    summeryText = ''\n",
    "    summary_sentences = doc._.textrank.summary(limit_phrases=2, limit_sentences=1)\n",
    "    return ' '.join([sent.text for sent in summary_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa697fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtree = ET.parse(dB_file)\n",
    "xroot = xtree.getroot()\n",
    "drugs = list(xroot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f50fe70a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 1/15235 [00:00<19:26, 13.06it/s]\n"
     ]
    }
   ],
   "source": [
    "drug_targets = []\n",
    "\n",
    "for i in tqdm(range(len(drugs))):\n",
    "    drug = drugs[i]\n",
    "    idDB = drug[0].text # Drug Bank ID\n",
    "    \n",
    "\n",
    "    for idx,feature in enumerate(drug):\n",
    "        if 'name' in str(feature): # drug name\n",
    "            drug_name = drug[idx].text\n",
    "        \n",
    "        if 'state' in str(feature):\n",
    "            drug_state = drug[idx].text\n",
    "\n",
    "        if 'classification' in str(feature): #type of drug   \n",
    "            drug_class_kingdom = list(drug[idx])[2].text\n",
    "            drug_class_superclass = list(drug[idx])[3].text\n",
    "            \n",
    "\n",
    "        if 'drug-interactions' in str(feature): #interaction other drugs\n",
    "            \n",
    "            drug_interaction_temp = '$'.join([di[2].text for di in list(drug[idx])])\n",
    "            sentence_temp = drug_interaction_temp.split('$')\n",
    "            drug_interaction =  [' '.join([word for word in word_tokenize(sentence) if not word in stop_words]) for sentence in sentence_temp]\n",
    "\n",
    "           \n",
    "\n",
    "        if 'pathways' in str(feature): #related pathways\n",
    "            \n",
    "            drug_pathway = [pathway[1].text \\\n",
    "                                    for pathway in list(drug[idx])]\n",
    "        \n",
    "\n",
    "        if 'targets' in str(feature): #if polypeptide, drug's targets\n",
    "            targets = list(drug[idx])\n",
    "            \n",
    "        \n",
    "        if 'toxicity' in str(feature):\n",
    "            \n",
    "                \n",
    "            drug_toxicity_temp = f\"${drug[idx].text}\"\n",
    "            sentences = drug_toxicity_temp.split('$')\n",
    "            drug_toxicity =  [' '.join([word for word in word_tokenize(sentence) if not word in stop_words]) for sentence in sentences]\n",
    "            \n",
    "            \n",
    "                \n",
    "        if 'food-interactions' in str(feature):\n",
    "            \n",
    "            drug_food_interactions_temp = '$'.join([di.text for di in list(drug[idx])])\n",
    "            sentences = drug_food_interactions_temp.split('$')\n",
    "            drug_food_interactions =  [' '.join([word for word in word_tokenize(sentence) if not word in stop_words]) for sentence in sentences]\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    # get all drug-related information in a dictionary\n",
    "    if [\"\"] in [drug_name, drug_class_kingdom, drug_class_superclass,drug_interaction,drug_pathway,drug_toxicity,drug_food_interactions]:\n",
    "        continue \n",
    "    \n",
    "    drug_dict = {\"id\":idDB,\n",
    "                \"name\":drug_name,\n",
    "                 \"state\":drug_state,\n",
    "                \"kgd\":drug_class_kingdom,\n",
    "                \"sclass\":drug_class_superclass,\n",
    "                \"itrc\":drug_interaction,\n",
    "                \"pathways\":drug_pathway,\n",
    "                \"toxicity\":drug_toxicity,\n",
    "                \"food_interactions\":drug_food_interactions}\n",
    "    drug = Drug(drug_dict)\n",
    "\n",
    "    # get information of polypeptide targets\n",
    "    if len(targets) > 0:\n",
    "        for target in targets:\n",
    "            idx_pep = None\n",
    "            # get indexes\n",
    "            for idx,feature in enumerate(target): # check features of targets\n",
    "                if 'organism' in str(feature):\n",
    "                    idx_org = idx\n",
    "                if 'name' in str(feature):\n",
    "                    idx_name = idx\n",
    "                if 'actions' in str(feature):\n",
    "                    idx_act = idx\n",
    "                if 'polypeptide' in str(feature):\n",
    "                    idx_pep = idx\n",
    "\n",
    "            # Get information for polypeptide\n",
    "            if target[idx_org].text == organism:\n",
    "\n",
    "                target_name = target[idx_name].text\n",
    "\n",
    "                actions = ';'.join([action.text\n",
    "                                    for action in list(target[idx_act])])\n",
    "\n",
    "                # Get information for polypeptide\n",
    "                if idx_pep is not None: #if there is polypeptide's info...\n",
    "                    for idx,feature in enumerate(target[idx_pep]):\n",
    "                        if 'gene-name' in str(feature):\n",
    "                            gene_name = target[idx_pep][idx].text\n",
    "                        if 'cellular-location' in str(feature):\n",
    "                            cell_loc = target[idx_pep][idx].text\n",
    "                        if 'external-identifiers' in str(feature):\n",
    "                            for ext_id in list(target[idx_pep][idx]):\n",
    "                                if ext_id[0].text == \"UniProtKB\":\n",
    "                                    uniprot = ext_id[1].text\n",
    "                else:\n",
    "                    gene_name = None\n",
    "                    action = None\n",
    "                    cell_loc = None\n",
    "                    uniprot = None\n",
    "\n",
    "                row = {\n",
    "                        \"drug_id\":drug.id,\n",
    "                        \"drug_name\":drug.name,\n",
    "                        \"drug_state\":drug.state,\n",
    "                        \"drug_kingdom\":drug.kingdom,\n",
    "                        \"drug_superclass\":drug.superclass,\n",
    "                        \"drug_interactions\":drug.interaction,\n",
    "                        \"drug_pathways\":drug.pathways,\n",
    "                        \"drug_toxicity\":drug.toxicity,\n",
    "                        \"drug_food-interactions\":drug_food_interactions,\n",
    "                        \"target_name\":target_name,\n",
    "                        \"target_uniprot\":uniprot,\n",
    "                        \"target_gene_name\":gene_name,\n",
    "                        \"action\":actions,\n",
    "                        \"cell_loc\":cell_loc}\n",
    "\n",
    "                drug_targets.append(row)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2457d094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdrug_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(saveFile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(drug_targets, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(drug_targets[1])\n",
    "\n",
    "with open(saveFile, 'w') as f:\n",
    "    json.dump(drug_targets, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c433cfb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Önce JSON dosyasını bir DataFrame'e yükleyin\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrug_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ardından DataFrame'i tekrar JSON dosyasına kaydedin\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmaller_file.json\u001b[39m\u001b[38;5;124m'\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\MAKINE_OGRENIMI_VE_UYGULAMALARI\\let1\\env\\lib\\site-packages\\pandas\\io\\json\\_json.py:760\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    758\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 760\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mC:\\MAKINE_OGRENIMI_VE_UYGULAMALARI\\let1\\env\\lib\\site-packages\\pandas\\io\\json\\_json.py:861\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 861\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32mC:\\MAKINE_OGRENIMI_VE_UYGULAMALARI\\let1\\env\\lib\\site-packages\\pandas\\io\\json\\_json.py:901\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    894\u001b[0m filepath_or_buffer \u001b[38;5;241m=\u001b[39m stringify_path(filepath_or_buffer)\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_url(filepath_or_buffer)\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_fsspec_url(filepath_or_buffer)\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    900\u001b[0m ):\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    916\u001b[0m ):\n",
      "File \u001b[1;32mC:\\MAKINE_OGRENIMI_VE_UYGULAMALARI\\let1\\env\\lib\\site-packages\\pandas\\io\\common.py:716\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    713\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    725\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mC:\\MAKINE_OGRENIMI_VE_UYGULAMALARI\\let1\\env\\lib\\site-packages\\pandas\\io\\common.py:456\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    454\u001b[0m ):\n\u001b[0;32m    455\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file path or buffer object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(filepath_or_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[0;32m    459\u001b[0m     filepath_or_buffer\u001b[38;5;241m=\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    460\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    464\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'list'>"
     ]
    }
   ],
   "source": [
    "# Önce JSON dosyasını bir DataFrame'e yükleyin\n",
    "df = pd.read_json(drug_targets)\n",
    "\n",
    "# Ardından DataFrame'i tekrar JSON dosyasına kaydedin\n",
    "df.to_json('smaller_file.json', orient='records', lines=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "drug_toxicity = []\n",
    "drug_food_interactions = []\n",
    "\n",
    "for i in tqdm(range(len(drugs))):\n",
    "    drug = drugs[i]\n",
    "    idDB = drug[0].text  # Drug Bank ID\n",
    "\n",
    "    \n",
    "    for idx, feature in enumerate(drug):\n",
    "\n",
    "        if 'toxicity' in str(feature):\n",
    "            drug_toxicity_temp = drug[idx].text if drug[idx].text else None\n",
    "            drug_toxicity.append(drug_toxicity_temp)\n",
    "\n",
    "        if 'food-interactions' in str(feature):\n",
    "            drug_food_interactions_temp = [fi.text for fi in list(drug[idx])] if list(drug[idx]) else None\n",
    "            drug_food_interactions.append(drug_food_interactions_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530aa207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "flat_list = [item for sublist in drug_food_interactions if sublist is not None for item in sublist]\n",
    "all_text = ' '.join(flat_list)\n",
    "\n",
    "\n",
    "# Kelimeleri ayırın ve küçük harfe çevirin\n",
    "words = [word.lower() for word in word_tokenize(all_text) if word.isalpha()]\n",
    "\n",
    "# Stopwords'leri kaldırın\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Kelimeleri sayın\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# En çok tekrar eden 100 kelimeyi bulun\n",
    "top_words = word_counts.most_common(1000)\n",
    "\n",
    "# Sonuçları görüntüleyin\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0893f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d02e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
